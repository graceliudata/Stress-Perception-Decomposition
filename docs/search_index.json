[["index.html", "Stress Perception Decomposition Chapter 1 Introduction", " Stress Perception Decomposition Hongling Liu, Megan Goldring, Xinrui Zhang and Yuxin Zhou 2020-12-13 Chapter 1 Introduction "],["data-sources.html", "Chapter 2 Data sources", " Chapter 2 Data sources This data comes from a study designed and run by Megan, who is also a PhD student in the Psychology Department. We confirmed with Professor Robbins that using this dataset is appropriate for the EDAV course. The main purpose of the study was to conduct a crossed random effects model in order to decompose variability in stress perception into person, stressor, and person by stressor components. To do so, we asked multiple people to provide ratings of how stressed and overwhelmed they would be to each of 60 vignettes designed to range in objective stress. Although the crossed random effect model is not part of our project for this class, visualizing and analyzing the raw data will be. In generating the vignettes of stressful events, we used the Daily Inventory of Stressful Events coding scheme. This scheme trains objective coders to rate each vignette. Two coders were trained and rated each vignette according to the DISE scheme; specifically how objectively severe the stressor is from 0=not at all to 4=extremely, the life category that the stressor falls into (i.e. home, work, etc), whether the stressor was chronic, whether it was a continuation of a stressor from a previous day, and whether the focus of involvement for the stressor was the self or someone else (i.e. a close family member). To collect the data, 157 undergraduate students at Columbia participated for course credit. The study took place in 3 parts: 1) a 15-minute pre-study baseline survey, 2) study session one, and 3) study session two. The baseline survey assessed relevant psychological constructs such as loneliness, chronic stress experiences, and stress mindsets, in addition to demographic variables such as participant year in school, race, family income, etc. The second part consisted of meeting a trained research assistant online via Zoom. Participants first provided informed consent, then completed 3 brief mood questionnaires (Profile of Mood States; assessing current mood, mood during COVID, and mood in general in their life), followed by responding to each of the 60 vignettes. Upon hearing the research assistant read each vignette out loud, participants indicated: 1) how stressed they would be from 0=not at all to 4=extremely, 2) how overwhelmed they would be from 0=not at all to 4= extremely, 3) whether they had experienced something like the event described in the vignette with 1=yes and 0=no 4) whether it was easy to imagine themselves in the scenario described in the vignette with 1=yes and 0= no. All data was therefore collected online via Qualtrics and it is all self-report survey data. The different data files contain different types of variables; either between-stressor measures of the vignettes (i.e. the objective codes), between-person measures of psychological traits and demographics (i.e. baseline data), or within-person stressor measures (i.e. the session 1 and session 2 data). Due to the nature of the study (i.e. Columbia students filling out the surveys for course credit), there is very little missingness and few errors. The only errors that we did find (and not resolve!) was that three of the vignettes had more questions than they were supposed to. We opted to simply remove those vignettes from analysis for the purpose of this class. "],["data-transformation.html", "Chapter 3 Data transformation", " Chapter 3 Data transformation There are four datafiles that we are working with: the objective code data file, the baseline data file, the mood measure data file (contains both sessions worth of data), and the vignette data file (also contains both sessions worth of data). We cleaned the baseline data in excel prior to inputting it in R; there are only a few clean ups that needed to happen: 1) eliminate rows 2 and 3 that contained meta data about the survey, 2) eliminate columns 2-15 because they contained irrelevant variables (i.e. longitude and latitude at which the survey was taken), and 3) eliminate the column with participant birthdays, as this is personally identifiable information and we only received IRB approval to disseminate and post completely anonymized data. The experimental data sessions were a little trickier. In order to partially randomize the order of the vignettes, participants were a priori randomly assigned to one of ten conditions, where all conditions contained the exact same vignettes but in a different order. Therefore, the structure of each of these datasets is the same first few columns (those obtaining informed consent and the mood measures) followed by the vignette questions that were in different orders depending on the datafile. Because of this, we had to read in 10 separate datafiles, restructure them, and merge them. We used the following code to do so: #load necessary libraries library(tidyverse) #Read in the data cond1 &lt;- read.csv(cond_1.csv) cond2 &lt;- read.csv(cond_2.csv) cond3 &lt;- read.csv(cond_3.csv) cond4 &lt;- read.csv(cond_4.csv) cond5 &lt;- read.csv(cond_5.csv) cond6 &lt;- read.csv(cond_6.csv) cond7 &lt;- read.csv(cond_7.csv) cond8 &lt;- read.csv(cond_8.csv) cond9 &lt;- read.csv(cond_9.csv) cond10 &lt;- read.csv(cond_10.csv) #Eliminate unecessary rows and columns (i.e. ip address and rows with question labels) cond1 &lt;- cond1[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond2 &lt;- cond2[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond3 &lt;- cond3[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond4 &lt;- cond4[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond5 &lt;- cond5[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond6 &lt;- cond6[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond7 &lt;- cond7[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond8 &lt;- cond8[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond9 &lt;- cond9[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] cond10 &lt;- cond10[-c(1:2),-c(1,2,3,4,5,8,9,10,11,12,13,14,15,16,17)] #re-label columns cond1 &lt;- plyr::rename(cond1, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond2 &lt;- plyr::rename(cond2, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond3 &lt;- plyr::rename(cond3, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond4 &lt;- plyr::rename(cond4, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond5 &lt;- plyr::rename(cond5, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond6 &lt;- plyr::rename(cond6, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond7 &lt;- plyr::rename(cond7, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond8 &lt;- plyr::rename(cond8, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond9 &lt;- plyr::rename(cond9, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) cond10 &lt;- plyr::rename(cond10, c(Duration..in.seconds.=Duration_sec, Q304=Consent, Q289=ID, Q290=Session)) #label condition cond1$Condition &lt;- 1 cond2$Condition &lt;- 2 cond3$Condition &lt;- 3 cond4$Condition &lt;- 4 cond5$Condition &lt;- 5 cond6$Condition &lt;- 6 cond7$Condition &lt;- 7 cond8$Condition &lt;- 8 cond9$Condition &lt;- 9 cond10$Condition &lt;- 10 #eliminate vignettes 6, 127, and 143 because something went wrong with those vignettes and we dont know what cond1 &lt;- cond1[,-c(which(str_detect(colnames(cond1), V6_)), which(str_detect(colnames(cond1), V127_)), which(str_detect(colnames(cond1), V143_)))] cond2 &lt;- cond2[,-c(which(str_detect(colnames(cond2), V6_)), which(str_detect(colnames(cond2), V127_)), which(str_detect(colnames(cond2), V143_)))] cond3 &lt;- cond3[,-c(which(str_detect(colnames(cond3), V6_)), which(str_detect(colnames(cond3), V127_)), which(str_detect(colnames(cond3), V143_)))] cond4 &lt;- cond4[,-c(which(str_detect(colnames(cond4), V6_)), which(str_detect(colnames(cond4), V127_)), which(str_detect(colnames(cond4), V143_)))] cond5 &lt;- cond5[,-c(which(str_detect(colnames(cond5), V6_)), which(str_detect(colnames(cond5), V127_)), which(str_detect(colnames(cond5), V143_)))] cond6 &lt;- cond6[,-c(which(str_detect(colnames(cond6), V6_)), which(str_detect(colnames(cond6), V127_)), which(str_detect(colnames(cond6), V143_)))] cond7 &lt;- cond7[,-c(which(str_detect(colnames(cond7), V6_)), which(str_detect(colnames(cond7), V127_)), which(str_detect(colnames(cond7), V143_)))] cond8 &lt;- cond8[,-c(which(str_detect(colnames(cond8), V6_)), which(str_detect(colnames(cond8), V127_)), which(str_detect(colnames(cond8), V143_)))] cond9 &lt;- cond9[,-c(which(str_detect(colnames(cond9), V6_)), which(str_detect(colnames(cond9), V127_)), which(str_detect(colnames(cond9), V143_)))] cond10 &lt;- cond10[,-c(which(str_detect(colnames(cond10), V6_)), which(str_detect(colnames(cond10), V127_)), which(str_detect(colnames(cond10), V143_)))] #separate poms measures and vignettes cond1_poms &lt;- cond1[,c(4:23,259)] cond1_vign &lt;- cond1[,c(4,5,37:259)] cond2_poms &lt;- cond2[,c(4:23,259)] cond2_vign &lt;- cond2[,c(4,5,37:259)] cond3_poms &lt;- cond3[,c(4:23,259)] cond3_vign &lt;- cond3[,c(4,5,37:259)] cond4_poms &lt;- cond4[,c(4:23,259)] cond4_vign &lt;- cond4[,c(4,5,37:259)] cond5_poms &lt;- cond5[,c(4:23,259)] cond5_vign &lt;- cond5[,c(4,5,37:259)] cond6_poms &lt;- cond6[,c(4:23,259)] cond6_vign &lt;- cond6[,c(4,5,37:259)] cond7_poms &lt;- cond7[,c(4:23,259)] cond7_vign &lt;- cond7[,c(4,5,37:259)] cond8_poms &lt;- cond8[,c(4:23,259)] cond8_vign &lt;- cond8[,c(4,5,37:259)] cond9_poms &lt;- cond9[,c(4:23,259)] cond9_vign &lt;- cond9[,c(4,5,37:259)] cond10_poms &lt;- cond10[,c(4:23,259)] cond10_vign &lt;- cond10[,c(4,5,37:259)] #make vignette dataset #merge together dat &lt;- dplyr::bind_rows(cond1_vign, cond2_vign) dat &lt;- dplyr::bind_rows(dat, cond3_vign) dat &lt;- dplyr::bind_rows(dat, cond4_vign) dat &lt;- dplyr::bind_rows(dat, cond5_vign) dat &lt;- dplyr::bind_rows(dat, cond6_vign) dat &lt;- dplyr::bind_rows(dat, cond7_vign) dat &lt;- dplyr::bind_rows(dat, cond8_vign) dat &lt;- dplyr::bind_rows(dat, cond9_vign) dat &lt;- dplyr::bind_rows(dat, cond10_vign) #eliminate pilot data and test runs dat\\(ID &lt;- as.numeric(dat\\)ID) dat &lt;- dat[which(dat$ID &lt; 999),] #wide to long datl &lt;- tidyr::gather(dat, key = vignette, value = score, 3:224) #save datafile saveRDS(datl, file = vignette_dat.RDS) #make poms datafile #merge together dat &lt;- dplyr::bind_rows(cond1_poms, cond2_poms) dat &lt;- dplyr::bind_rows(dat, cond3_poms) dat &lt;- dplyr::bind_rows(dat, cond4_poms) dat &lt;- dplyr::bind_rows(dat, cond5_poms) dat &lt;- dplyr::bind_rows(dat, cond6_poms) dat &lt;- dplyr::bind_rows(dat, cond7_poms) dat &lt;- dplyr::bind_rows(dat, cond8_poms) dat &lt;- dplyr::bind_rows(dat, cond9_poms) dat &lt;- dplyr::bind_rows(dat, cond10_poms) #eliminate pilot data and test runs dat\\(ID &lt;- as.numeric(dat\\)ID) dat &lt;- dat[which(dat$ID &lt; 999),] #wide to long datl &lt;- tidyr::gather(dat, key = item, value = score, 3:20) #save datafile saveRDS(datl, file = poms_dat.RDS) "],["missing-values.html", "Chapter 4 Missing Values", " Chapter 4 Missing Values Our data missing analysis involves 3 data sets: Mood data, Vignette data and Baseline data. ## missing % ## ID 0 0 ## Session 0 0 ## vignette 0 0 ## score 0 0 ## missing % ## ID 0 0 ## Session 0 0 ## vignette 0 0 ## score 0 0 Among those 3 data sets, only Baseline data has different percentage of values missing. For column patterns, we can see that the top 3 variables have the most missing data: non_heteronormative_3_TEXT, gender_idetity_5_TEXT and race_12_TEXT This is quite reasonable since these variables are used to store the explanation part if people choose the other option for variable non_heteronormative, gender_idetity and race, respectively. Therefore, the reason why these 3 columns has leading missing values is people choose common options but rarely the other option for non_heteronormative, gender_idetity and race variables. ## missing % ## non_heteronormative_3_TEXT 201 100 ## gender_idetity_5_TEXT 199 99 ## race_12_TEXT 197 98 ## mother_education 62 31 ## father_education 62 31 ## period 58 29 ## gender_idetity 47 23 ## contraceptive 26 13 ## loc2 20 10 ## household_income 20 10 ## loc1 19 9 ## loc3 19 9 ## coherence6 19 9 ## sibling_age 19 9 ## focus1 18 9 ## focus2 18 9 ## focus3 18 9 ## focus4 18 9 ## focus5 18 9 ## focus6 18 9 For row patterns, with each row represents a student survey results, we found that there were 17 out of 201 students that only fill out a few of survey question and then quit. In fact, we decide to remove those who did not fill out at least 75% of the survey for data cleaning. ## [1] 4 4 4 6 5 4 5 4 4 4 4 4 4 4 4 5 5 4 4 4 4 4 4 3 179 6 ## [27] 180 4 4 4 5 178 5 5 7 5 5 4 7 5 5 5 5 5 5 5 6 178 6 6 5 5 ## [53] 5 5 6 5 4 6 5 7 6 5 5 5 5 6 5 5 8 5 7 5 179 3 3 6 75 3 ## [79] 3 3 6 3 3 3 3 3 180 3 179 3 3 3 3 3 3 4 3 3 178 178 3 4 3 4 ## [105] 165 3 3 3 3 3 3 3 3 3 3 179 3 3 180 2 3 3 4 3 3 3 3 4 3 3 ## [131] 6 178 8 3 3 154 3 3 5 178 3 4 3 3 3 3 4 3 4 4 3 5 3 3 3 3 ## [157] 3 2 3 3 3 5 5 3 3 4 3 3 3 3 179 3 3 3 1 4 3 4 6 3 3 5 ## [183] 3 3 3 3 3 4 180 4 3 4 4 3 4 3 4 3 4 4 4 Here is a heatmap for row/column missing pattern, where x-axis for columns (survey questions), y-axis for row index (id for students) 1.While yellow cells indicates missing data, we can see patterns of 3 vertical yellow lines (top 3 variables with missing values) and 17 horizontal yellow lines (17 students miss more than 75% questions) 2.There is another pattern that students with id 31-73 have missing values for variable father_education and mother_education. "],["results.html", "Chapter 5 Results", " Chapter 5 Results One major question of interest in this study was whether participants self-reports of how stressed they would be in each scenario corresponds with objective coder ratings of the severity of those scenarios, as coded by the Daily Inventory of Stressful Events (DISE) scheme (Midus, 1994). A priori, we expected that participant ratings of the stressfulness of the events would correspond with the objective coders ratings of the severity of the events. Both the subjective responses and the objective codes ranged from 0=not at all to 4=extremely. We were also interested in investing whether variance in subjective reports depended on the objective codes; for example, whether scenarios in the 2-3 range in objective severity were more variable while those in the high and low ranges of severity exhibited less variability. To explore these ideas, we build six histogram-style ridge plots, three for each session (session 1 or session 2 of the study) and one for each measure of stress (how stressed did you feel? how overwhelmed did you feel? and an aggregate score that is the mean of how stressed and overwhelmed participants felt). In the plots below, the x-axis is subjective ratings of stressed, overwhelmed, and the aggregate and the y-axis is the ID number for the vignettes. The y-axis is ordered such that vignettes at the top are the most objectively severe scenarios (4=extremely stressful) and decrease down the axis to the bottom, which contains vignettes rated as least objectively severe (0=not at all stressful). The objective severity of the scenarios is also color-coded, as depicted in a legend to the right of the plots. We have several key takeaways from these visualizations. All discussion refers to all three ways of operationalizing stress: how stressed, how overwhelmed, and the aggregate of stressed and overwhelmed that paticipants reported. First, we see that participants generally report that they would be less stressed than expected by the objective coding scheme; many participants rate scenarios that are objectively a 1 as a 0, those that are objectively a 2 as a 1, and so on. However, the under-reporting is most severe for the low-range objective stressors (the 1s and 2s). At the same time, we do see the expected general increase in mean subjective ratings of stress across the objective range, with more objectively severe stressors being rated as more subjectively stressors. Next, we observe less variability in the high and low ranges of the scale, particularly in the 0 range. This implies that participants generally perceive very non-severe and very severe stressors as consensually stressful (i.e. little disagreement about the stressfulness of these scenarios), while subjective interpretation and therefore variability emerges in the 2 and 3 range of the objective scale. We think this visualization does a nice job of laying out the correspondence between objetive coding of the stressors and subjective interpretations of them. ############################################# The vignettes are designed with different severity. The termiology severity means how sever the potential consequence resulted from the conditions described in the vignettes may be.The severity is leveled into 4 levels, where 0 indicates the least severe and 4 stands for the most severe. Also, the vignettes are designed with different contents that are categorized into 7 categories: Discrimination,Finances,Health/Accident, Home, Interpersonal Tension, Misc, Work/Education. We want to see how people react under different Severity and different contents. Reaction v.s. Severity From mosaic plot, we can see higher severity contributes more to the higher score, that is, the vignette with higher severity yield more high scores. From the bar plot, we can see an approximate V shape. With lower severity(1-3), the counts of scores is in an approximate descending order(score 0 counts most, score 4 counts least). With higher severity(3-4), the counts of scores is in an approximate ascending order(score 0 counts least, score 4 counts most). The bar plot can explain the proportion pattern presents in the mosaic plot consistently. Reaction v.s. Content The mosaic plots show clearly that the higher score, the Discrimination and Finance contributes more. In other words, the vignettes with contents related to discrimination and finance yield more high scores. From bar plots, under contents Health/Accident, Home, Interpersonal Tension, Misc, and Work/Education, the counts of the score presents an approximate descending order(score 0 counts most, score 4 counts last). However, for contents Discrimination and Finance, the counts of score is relatively even or in an ascending order, that is the reason why this two contents contribute more in higher score. Another pattern we can find from mosaic plot is that the contribution of Health/Accident across the score basically stays same. ############################################# We also would like to see how peoples baseline condition may have impacts on peoples reaction to stress. We have sex assignment, household income, loneliness, relationship with a peoples close ones these four factors seleted. sex assignment is simply clustered as Male and Female. Other three factors are seperated in 3 levels respectively. By doing parallel coordinate graph, our goal is to see if clustering patterns exist for each factor. The parallel columns state for each vignette, ordered from most severe to least severe(left are those stressful conditions that may lead to the most severe consequence, and right are those that may lead to the least severe consequence). From graphs below, the only clustering pattern we can see is from sex assignment. It shows that males are more likely to state that they are good at dealing with those stressful conditions that may lead to severe consequence. other three fatcors present no evident impact on peoples reaction. ############################################# We were also interested in visualizing the distribution of responses to the two questions asked for each scenario: have you experienced something like this before? (1=yes) and was it easy to imagine yourself in this scenario? (1=yes). We therefore calculated the proportion of participants who responded yes to each question and visualized a box and whisker plot of those proportions. We chose box and whisker plots because we are interested in finding outliers; in the next iteration of the study we will likely exclude scenarios that were extreme in terms of describing events that the sample hadnt experienced and being difficult to imagine oneself in the scenario. These visualizations reveal several important things about the study. First, participants generally indicated that it was easy to imagine themselves in the scenarios (means for both session were above 90%), even though they hadnt necessarily been in situations exactly like them before (means for both sessions around 45%-50%). However, there were a few scenarios that participants found difficult to imagine: scnarios 165 and 48. In the next iteration of the study, we will likely remove these scenarios and replace them with situations that are more commonly experienced by undergraduate students. "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component One of the questions were interested in is how our students react to different types of content and what kinds of issues cause them more stress than others. We created a stacked bar chart to illustrate our findings: Based on our stacked bar chart, we could tell that there are more vignettes in categories like Work/Education, Discrimination and Misc, and there are relatively less vignettes in Health/Accicdent than other categories. For content in regard to Discrimination and Finances, there are a larger portion of students that feel high level of stress. This founding makes sense since college students are typically young adults that lack financial literacy and economic stability, and race has always been a sensitive issue that causes stress on people and especially young people. There is a larger portion of students that are indifferent about content related to Health/Accident, which also makes sense considering that college students are typically in good health and health may be least of their worries. However, it suprises me that there is also a sightly larger portion of students that are indifferent about content related to Discrimination, which may indicate that some people may be less conscious about issues relating to race, although there are also a large percentage of people that are very conscious about that. The survey in Session 2 seems to reflect an overall lower level of stress comparing to the results from Session 1, which may be effected by external factors such as the time period where our survey were conducted. "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion "]]
